{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf0f196-0b6c-48e3-89d6-f7752e2a2842",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.What is Logistic Regression, and how does it differ from Linear Regression.\n",
    "Logistic Regression is a classification algorithm used to predict categorical outcomes (e.g., Yes/No, 0/1, True/False).\n",
    "Linear Regression predicts continuous values, \n",
    "whereas Logistic Regression predicts probabilities and maps them to discrete classes using a threshold (typically 0.5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66b56a1-f5cf-4aa9-91a1-18afddb91331",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.What is the mathematical equation of Logistic Regression.\n",
    "The logistic regression model is given by:\n",
    "\n",
    "ùëù\n",
    "(\n",
    "ùëã\n",
    ")\n",
    "=\n",
    "1\n",
    "1\n",
    "+\n",
    "ùëí\n",
    "‚àí\n",
    "(\n",
    "ùõΩ\n",
    "0\n",
    "+\n",
    "ùõΩ\n",
    "1\n",
    "ùëã\n",
    "1\n",
    "+\n",
    "ùõΩ\n",
    "2\n",
    "ùëã\n",
    "2\n",
    "+\n",
    ".\n",
    ".\n",
    ".\n",
    "+\n",
    "ùõΩ\n",
    "ùëõ\n",
    "ùëã\n",
    "ùëõ\n",
    ")\n",
    "p(X)= \n",
    "1+e \n",
    "‚àí(Œ≤ \n",
    "0\n",
    "‚Äã\n",
    " +Œ≤ \n",
    "1\n",
    "‚Äã\n",
    " X \n",
    "1\n",
    "‚Äã\n",
    " +Œ≤ \n",
    "2\n",
    "‚Äã\n",
    " X \n",
    "2\n",
    "‚Äã\n",
    " +...+Œ≤ \n",
    "n\n",
    "‚Äã\n",
    " X \n",
    "n\n",
    "‚Äã\n",
    " )\n",
    " \n",
    "1\n",
    "‚Äã\n",
    " \n",
    "where \n",
    "ùëù\n",
    "(\n",
    "ùëã\n",
    ")\n",
    "p(X) is the probability of the positive class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fcb8b9-e8ca-4367-ba81-0bf35717ba43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.Why do we use the Sigmoid function in Logistic Regression.\n",
    "The Sigmoid function:\n",
    "\n",
    "ùúé\n",
    "(\n",
    "ùëß\n",
    ")\n",
    "=\n",
    "1\n",
    "1\n",
    "+\n",
    "ùëí\n",
    "‚àí\n",
    "ùëß\n",
    "œÉ(z)= \n",
    "1+e \n",
    "‚àíz\n",
    " \n",
    "1\n",
    "‚Äã\n",
    " \n",
    "Converts any real-valued number into a probability between 0 and 1.\n",
    "Helps in binary classification by defining a clear decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aafb5fb-900f-48ff-a427-2a4d44057662",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.What is the cost function of Logistic Regression.\n",
    "Logistic Regression uses log loss (cross-entropy loss):\n",
    "\n",
    "ùêΩ\n",
    "(\n",
    "ùúÉ\n",
    ")\n",
    "=\n",
    "‚àí\n",
    "1\n",
    "ùëö\n",
    "‚àë\n",
    "ùëñ\n",
    "=\n",
    "1\n",
    "ùëö\n",
    "[\n",
    "ùë¶\n",
    "ùëñ\n",
    "log\n",
    "‚Å°\n",
    "(\n",
    "‚Ñé\n",
    "ùúÉ\n",
    "(\n",
    "ùë•\n",
    "ùëñ\n",
    ")\n",
    ")\n",
    "+\n",
    "(\n",
    "1\n",
    "‚àí\n",
    "ùë¶\n",
    "ùëñ\n",
    ")\n",
    "log\n",
    "‚Å°\n",
    "(\n",
    "1\n",
    "‚àí\n",
    "‚Ñé\n",
    "ùúÉ\n",
    "(\n",
    "ùë•\n",
    "ùëñ\n",
    ")\n",
    ")\n",
    "]\n",
    "J(Œ∏)=‚àí \n",
    "m\n",
    "1\n",
    "‚Äã\n",
    "  \n",
    "i=1\n",
    "‚àë\n",
    "m\n",
    "‚Äã\n",
    " [y \n",
    "i\n",
    "‚Äã\n",
    " log(h \n",
    "Œ∏\n",
    "‚Äã\n",
    " (x \n",
    "i\n",
    "‚Äã\n",
    " ))+(1‚àíy \n",
    "i\n",
    "‚Äã\n",
    " )log(1‚àíh \n",
    "Œ∏\n",
    "‚Äã\n",
    " (x \n",
    "i\n",
    "‚Äã\n",
    " ))]\n",
    "where \n",
    "‚Ñé\n",
    "ùúÉ\n",
    "(\n",
    "ùë•\n",
    ")\n",
    "h \n",
    "Œ∏\n",
    "‚Äã\n",
    " (x) is the predicted probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a474f90a-6b54-4225-aa47-652b31b20a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5.What is Regularization in Logistic Regression? Why is it needed.\n",
    "Regularization prevents overfitting by penalizing large coefficients in the model.\n",
    "\n",
    "L1 Regularization (Lasso): Shrinks some coefficients to zero, performing feature selection.\n",
    "L2 Regularization (Ridge): Shrinks all coefficients but does not set any to zero.\n",
    "Elastic Net: A mix of L1 and L2 regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23e5cb4-7bf5-413a-92e3-32494a3a2bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6.Explain the difference between Lasso, Ridge, and Elastic Net regression.\n",
    "Lasso (L1): Can shrink coefficients to zero, performing feature selection.\n",
    "Ridge (L2): Shrinks coefficients but retains all features.\n",
    "Elastic Net: A combination of both, balancing feature selection and coefficient shrinkage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ece0ed-b0ad-4e54-949e-44ce43bf319f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7.When should we use Elastic Net instead of Lasso or Ridge.\n",
    "Use Elastic Net when:\n",
    "\n",
    "There are correlated features (Lasso struggles in such cases).\n",
    "You need both regularization and feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8acad2-130a-42ba-b13e-092aa204704d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8.What is the impact of the regularization parameter (Œª) in Logistic Regression.\n",
    "High \n",
    "ùúÜ\n",
    "Œª ‚Üí More regularization ‚Üí Simpler model but higher bias.\n",
    "Low \n",
    "ùúÜ\n",
    "Œª ‚Üí Less regularization ‚Üí More complex model but higher variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e4aac3-21bb-4fcf-bcbd-cc7365d4b706",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9.What are the key assumptions of Logistic Regression.\n",
    "Linear relationship between independent variables and the log-odds.\n",
    "No multicollinearity among independent variables.\n",
    "Large sample size for stable estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d08daeb-d878-4e74-a9f0-5a47a855155f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10.What are some alternatives to Logistic Regression for classification tasks.\n",
    "Decision Trees\n",
    "Random Forests\n",
    "Support Vector Machines (SVM)\n",
    "Gradient Boosting (XGBoost, LightGBM, CatBoost)\n",
    "Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b36e522-4088-44ca-93cc-9d8aa55bf2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#11.What are Classification Evaluation Metrics.\n",
    "Accuracy\n",
    "Precision, Recall, and F1-Score\n",
    "ROC-AUC Curve\n",
    "Log-Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c3d8df-34ee-47aa-9033-d747d6ea65f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#12.How does class imbalance affect Logistic Regression.\n",
    "Leads to biased predictions favoring the majority class.\n",
    "Solutions: Oversampling, Undersampling, Class-weighted loss function, SMOTE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdf07f7-3f05-4cbe-adf8-c2dbbd137481",
   "metadata": {},
   "outputs": [],
   "source": [
    "#13.What is Hyperparameter Tuning in Logistic Regression.\n",
    "Optimizing parameters like regularization strength (\n",
    "ùúÜ\n",
    "Œª), solver choice, etc.\n",
    "Methods: Grid Search, Random Search, Bayesian Optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11d44ae-8841-4d99-9e78-4ed26f8f51c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#14.What are different solvers in Logistic Regression? Which one should be used.\n",
    "lbfgs (default, good for small to medium datasets)\n",
    "liblinear (good for small datasets, supports L1)\n",
    "saga (best for large datasets, supports L1, L2, Elastic Net)\n",
    "newton-cg (useful for high-dimensional data)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b965d1a6-2e1a-4138-8846-47a9727e8006",
   "metadata": {},
   "outputs": [],
   "source": [
    "#15.How is Logistic Regression extended for multiclass classification.\n",
    "One-vs-Rest (OvR): Fits multiple binary classifiers for each class.\n",
    "Softmax Regression (Multinomial Logistic Regression): Generalizes logistic regression to multi-class problems \n",
    "by assigning probabilities to all classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeda8372-46c6-4d3f-b6cc-51dca24a2c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#16.What are the advantages and disadvantages of Logistic Regression.\n",
    "#Advantages>>\n",
    "Simple and Interpretable ‚Äì Easy to implement and interpret.\n",
    "Probabilistic Output ‚Äì Provides probability estimates for classification.\n",
    "Efficient for Small Datasets ‚Äì Works well when the number of observations is not very large.\n",
    "Feature Importance ‚Äì Coefficients help understand feature significance.\n",
    "Less Prone to Overfitting ‚Äì Regularization (L1, L2) helps prevent overfitting.\n",
    "Works Well with Linearly Separable Data ‚Äì Performs effectively when data has a clear linear boundary.\n",
    "#Disadvantages>>\n",
    "Limited to Linearly Separable Problems ‚Äì Does not perform well when the data is non-linearly separable.\n",
    "Sensitive to Outliers ‚Äì Outliers can heavily influence the model.\n",
    "Feature Engineering Required ‚Äì Requires meaningful features for good performance.\n",
    "Multicollinearity Issues ‚Äì Correlated features can lead to unreliable coefficient estimates.\n",
    "Not Suitable for Complex Relationships ‚Äì May not capture complex patterns like deep learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7ba310-7cb0-4fc8-8900-0b0d626d3bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#17.What are some use cases of Logistic Regression.\n",
    "Medical Diagnosis ‚Äì Predicting diseases like diabetes or cancer.\n",
    "Credit Scoring ‚Äì Assessing loan default risks.\n",
    "Spam Detection ‚Äì Classifying emails as spam or not spam.\n",
    "Customer Churn Prediction ‚Äì Identifying customers likely to leave a service.\n",
    "Fraud Detection ‚Äì Identifying fraudulent transactions.\n",
    "Marketing Campaign Response ‚Äì Predicting whether a customer will respond to an offer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957bb5c0-2b6b-4b19-a17b-3b1372d4ff99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#18.What is the difference between Softmax Regression and Logistic Regression.\n",
    "\n",
    "Feature\t Logistic Regression\t         Softmax Regression\n",
    "Type\t Binary classification\t         Multiclass classification\n",
    "Output\t Single probability (sigmoid)\t Multiple probabilities (softmax)\n",
    "Formula\t\n",
    "ùëÉ\n",
    "(\n",
    "ùë¶\n",
    "=\n",
    "1\n",
    ")\n",
    "=\n",
    "1\n",
    "1\n",
    "+\n",
    "ùëí\n",
    "‚àí\n",
    "(\n",
    "ùë§\n",
    "ùëã\n",
    "+\n",
    "ùëè\n",
    ")\n",
    "P(y=1)= \n",
    "1+e \n",
    "‚àí(wX+b)\n",
    " \n",
    "1\n",
    "‚Äã\n",
    " \t\n",
    "ùëÉ\n",
    "(\n",
    "ùë¶\n",
    "=\n",
    "ùëñ\n",
    ")\n",
    "=\n",
    "ùëí\n",
    "ùë§\n",
    "ùëñ\n",
    "ùëã\n",
    "‚àë\n",
    "ùëí\n",
    "ùë§\n",
    "ùëó\n",
    "ùëã\n",
    "P(y=i)= \n",
    "‚àëe \n",
    "w \n",
    "j\n",
    "‚Äã\n",
    " X\n",
    " \n",
    "e \n",
    "w \n",
    "i\n",
    "‚Äã\n",
    " X\n",
    " \n",
    "‚Äã\n",
    " \n",
    "Decision Rule\tThreshold-based (0.5)\tArgmax (highest probability)\n",
    "Use Case\t2-class problems\tMulticlass problems (more than 2 classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9133859c-7fa6-46d8-b64c-73628771f72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#19.How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification.\n",
    "One-vs-Rest (OvR):\n",
    "\n",
    "Trains one classifier per class (each class vs all others).\n",
    "Suitable when classes are imbalanced or when interpretability is needed.\n",
    "Computationally efficient for a large number of classes.\n",
    "Works better if classes are not mutually exclusive.\n",
    "Softmax Regression (Multinomial):\n",
    "\n",
    "Trains a single model for all classes using the softmax function.\n",
    "More probabilistically sound and works well when classes are mutually exclusive.\n",
    "Preferred when all classes are needed in one prediction (e.g., digit recognition).\n",
    "Can be computationally expensive for a high number of classes.\n",
    "Choosing Between OvR and Softmax:\n",
    "\n",
    "Use Softmax when all classes are disjoint and need to be predicted together.\n",
    "Use OvR when you want individual probabilities and classes are not necessarily mutually exclusive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c033ae5e-db21-4e1e-b0ae-c564de299b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#20.How do we interpret coefficients in Logistic Regression?\n",
    "Coefficients (ùõΩŒ≤) represent the log-odds change for a one-unit increase in the predictor variable.\n",
    "The odds ratio is given by \n",
    "ùëí\n",
    "ùõΩ\n",
    "e \n",
    "Œ≤\n",
    " , which tells how the odds of the event change with a one-unit increase in the predictor.\n",
    "A positive coefficient means higher probability of the event occurring.\n",
    "A negative coefficient means lower probability of the event occurring.\n",
    "Example: If \n",
    "ùõΩ\n",
    "=\n",
    "0.7\n",
    "Œ≤=0.7, then odds increase by \n",
    "ùëí\n",
    "0.7\n",
    "‚âà\n",
    "2\n",
    "e \n",
    "0.7\n",
    " ‚âà2 times for a one-unit increase in the predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb90a15-a003-49a7-83c9-8ef17ffcb065",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PRACTICAL EXAMPLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41a7c3b0-5a58-44e4-9c3b-4f191e03cf34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "#1.Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic\n",
    "#Regression, and prints the model accuracy.\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply Logistic Regression\n",
    "model = LogisticRegression(max_iter=200)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and print accuracy\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Model Accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34723a16-7884-441a-9fbe-063bc775bfd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 Regularization Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "#2.Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1')\n",
    "#and print the model accuracy.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Apply L1 Regularization\n",
    "model_l1 = LogisticRegression(penalty='l1', solver='liblinear', max_iter=200)\n",
    "model_l1.fit(X_train, y_train)\n",
    "\n",
    "# Predict and print accuracy\n",
    "y_pred_l1 = model_l1.predict(X_test)\n",
    "print(\"L1 Regularization Accuracy:\", accuracy_score(y_test, y_pred_l1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dd0bb1c-a331-4fb4-a38e-4033be3b787d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 Regularization Accuracy: 1.0\n",
      "Model Coefficients: [[-0.3969917   0.96041003 -2.37401669 -1.00307592]\n",
      " [ 0.51273386 -0.25339633 -0.21526687 -0.76916219]\n",
      " [-0.11574216 -0.7070137   2.58928356  1.77223811]]\n"
     ]
    }
   ],
   "source": [
    "#3.Write a Python program to train Logistic Regression with L2 regularization (Ridge) using\n",
    "#LogisticRegression(penalty='l2'). Print model accuracy and coefficients.\n",
    "# Apply L2 Regularization\n",
    "model_l2 = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=200)\n",
    "model_l2.fit(X_train, y_train)\n",
    "\n",
    "# Predict and print accuracy\n",
    "y_pred_l2 = model_l2.predict(X_test)\n",
    "print(\"L2 Regularization Accuracy:\", accuracy_score(y_test, y_pred_l2))\n",
    "\n",
    "# Print model coefficients\n",
    "print(\"Model Coefficients:\", model_l2.coef_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af2236a9-5bfd-40f6-8cd8-5eaceabcae07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elastic Net Accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC DOCTOR\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#4.Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet').\n",
    "# Apply Elastic Net Regularization\n",
    "model_en = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, max_iter=200)\n",
    "model_en.fit(X_train, y_train)\n",
    "\n",
    "# Predict and print accuracy\n",
    "y_pred_en = model_en.predict(X_test)\n",
    "print(\"Elastic Net Accuracy:\", accuracy_score(y_test, y_pred_en))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4770f2a7-672b-49a4-9437-43c9b35bf8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OVR Classification Accuracy: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "#5.Write a Python program to train a Logistic Regression model for multiclass classification using\n",
    "#multi_class='ovr'.\n",
    "# Apply One-vs-Rest (OVR) Strategy\n",
    "model_ovr = LogisticRegression(multi_class='ovr', solver='lbfgs', max_iter=200)\n",
    "model_ovr.fit(X_train, y_train)\n",
    "\n",
    "# Predict and print accuracy\n",
    "y_pred_ovr = model_ovr.predict(X_test)\n",
    "print(\"OVR Classification Accuracy:\", accuracy_score(y_test, y_pred_ovr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dcc609-feed-4a92-8631-68139b37b186",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6.Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic\n",
    "#Regression. Print the best parameters and accuracy.\n",
    "\n",
    "\u001bfrom sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define hyperparameters to tune\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear']\n",
    "}\n",
    "\n",
    "# Apply GridSearchCV\n",
    "grid_search = GridSearchCV(LogisticRegression(max_iter=200), param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print best parameters and accuracy\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Accuracy:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "839a3181-bc8c-4351-9043-8c983d00a02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stratified K-Fold Average Accuracy: 0.9666666666666668\n"
     ]
    }
   ],
   "source": [
    "#7.Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the\n",
    "#average accuracy.\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "# Apply Stratified K-Fold Cross-Validation\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores = cross_val_score(LogisticRegression(max_iter=200), X, y, cv=skf)\n",
    "\n",
    "# Print average accuracy\n",
    "print(\"Stratified K-Fold Average Accuracy:\", scores.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00c1276-cb91-4110-a3ff-191a728efec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8.Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its\n",
    "#accuracy.\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset from CSV (modify 'your_dataset.csv' accordingly)\n",
    "df = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# Assume the last column is the target variable\n",
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply Logistic Regression\n",
    "model_csv = LogisticRegression(max_iter=200)\n",
    "model_csv.fit(X_train, y_train)\n",
    "\n",
    "# Predict and print accuracy\n",
    "y_pred_csv = model_csv.predict(X_test)\n",
    "print(\"CSV Data Accuracy:\", accuracy_score(y_test, y_pred_csv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68bd8b62-0ab4-47cb-8e28-4a3915e7861b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'solver': 'saga', 'penalty': 'l1', 'C': 11.288378916846883}\n",
      "Accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC DOCTOR\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "30 fits failed out of a total of 50.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\PC DOCTOR\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\PC DOCTOR\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1291, in fit\n",
      "    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, prefer=prefer)(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\PC DOCTOR\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 63, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\PC DOCTOR\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py\", line 1085, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\PC DOCTOR\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py\", line 901, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\PC DOCTOR\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py\", line 819, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\PC DOCTOR\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\PC DOCTOR\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py\", line 597, in __init__\n",
      "    self.results = batch()\n",
      "                   ^^^^^^^\n",
      "  File \"C:\\Users\\PC DOCTOR\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py\", line 288, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\PC DOCTOR\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py\", line 288, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\PC DOCTOR\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 123, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\PC DOCTOR\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 521, in _logistic_regression_path\n",
      "    alpha = (1.0 / C) * (1 - l1_ratio)\n",
      "                         ~~^~~~~~~~~~\n",
      "TypeError: unsupported operand type(s) for -: 'int' and 'NoneType'\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\PC DOCTOR\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\PC DOCTOR\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\PC DOCTOR\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 64, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\PC DOCTOR\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.83333333        nan        nan 0.94166667 0.65833333        nan\n",
      " 0.95833333        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#9.Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in\n",
    "#Logistic Regression. Print the best parameters and accuracy.\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'C': np.logspace(-4, 4, 20),\n",
    "    'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "    'solver': ['liblinear', 'saga']\n",
    "}\n",
    "\n",
    "# Apply RandomizedSearchCV\n",
    "log_reg = LogisticRegression(max_iter=5000)\n",
    "random_search = RandomizedSearchCV(log_reg, param_grid, cv=5, n_iter=10, scoring='accuracy', random_state=42)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Print best parameters and accuracy\n",
    "best_model = random_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "467ac747-c275-4cbe-8f3a-969d31f668aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-vs-One (OvO) Logistic Regression Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "#10.EHM Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy.\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train One-vs-One (OvO) Logistic Regression\n",
    "ovo_clf = OneVsOneClassifier(LogisticRegression(max_iter=5000))\n",
    "ovo_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate accuracy\n",
    "y_pred = ovo_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"One-vs-One (OvO) Logistic Regression Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "182bf63c-b693-4f12-aad0-2c089811683e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.855\n"
     ]
    }
   ],
   "source": [
    "#11.EEM Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary\n",
    "#classification.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# Generate binary classification data\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(5, 4))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Print accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be169023-8e3f-4208-a756-21b0b324ea42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.87, Recall: 0.82, F1-Score: 0.84\n"
     ]
    }
   ],
   "source": [
    "#12.Write a Python program to train a Logistic Regression model and evaluate its performance using Precision,\n",
    "#Recall, and F1-Score.\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Generate sample data\n",
    "X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Precision: {precision:.2f}, Recall: {recall:.2f}, F1-Score: {f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bb9587b-543c-4eef-aef2-d36632df1356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.41, Recall: 0.75, F1-Score: 0.53\n"
     ]
    }
   ],
   "source": [
    "#13.E\u0013M Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to\n",
    "#improve model performance.\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "# Generate imbalanced data\n",
    "X, y = make_classification(n_samples=1000, weights=[0.9, 0.1], random_state=42)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model with class weights\n",
    "model = LogisticRegression(class_weight='balanced')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Precision: {precision:.2f}, Recall: {recall:.2f}, F1-Score: {f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1903629d-79c4-4f45-9841-5f757eb6e7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC DOCTOR\\AppData\\Local\\Temp\\ipykernel_7060\\1110420232.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['Age'] = imputer.fit_transform(X[['Age']])\n",
      "C:\\Users\\PC DOCTOR\\AppData\\Local\\Temp\\ipykernel_7060\\1110420232.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['Sex'] = LabelEncoder().fit_transform(X['Sex'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.80\n"
     ]
    }
   ],
   "source": [
    "#14.Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and\n",
    "#evaluate performance.\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load dataset\n",
    "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Select features and target\n",
    "X = df[['Pclass', 'Sex', 'Age', 'Fare']]\n",
    "y = df['Survived']\n",
    "\n",
    "# Handle missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X['Age'] = imputer.fit_transform(X[['Age']])\n",
    "\n",
    "# Encode categorical data\n",
    "X['Sex'] = LabelEncoder().fit_transform(X['Sex'])\n",
    "\n",
    "# Split and train model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = model.score(X_test, y_test)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7176bc59-b2e9-4370-89c2-5bb9c65c8ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy without Scaling: 0.80\n",
      "Accuracy with Scaling: 0.80\n"
     ]
    }
   ],
   "source": [
    "#15.Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression\n",
    "#model. Evaluate its accuracy and compare results with and without scaling.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train model with scaled data\n",
    "model_scaled = LogisticRegression()\n",
    "model_scaled.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy_scaled = model_scaled.score(X_test_scaled, y_test)\n",
    "accuracy_unscaled = model.score(X_test, y_test)\n",
    "\n",
    "print(f\"Accuracy without Scaling: {accuracy_unscaled:.2f}\")\n",
    "print(f\"Accuracy with Scaling: {accuracy_scaled:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c107350-8b29-4f54-a8b6-12954194695e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC Score: 0.87\n"
     ]
    }
   ],
   "source": [
    "#16.Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Train model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities\n",
    "y_probs = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate ROC-AUC score\n",
    "roc_auc = roc_auc_score(y_test, y_probs)\n",
    "print(f\"ROC-AUC Score: {roc_auc:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0867360d-d1bb-446d-b891-1d6352f30972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.80\n"
     ]
    }
   ],
   "source": [
    "#17.Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate\n",
    "#accuracy\n",
    "# Train model with C=0.5\n",
    "model = LogisticRegression(C=0.5)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = model.score(X_test, y_test)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d7c0ebe-aee7-4580-a38f-82323715a6ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Feature  Coefficient\n",
      "3    Fare     0.001158\n",
      "2     Age    -0.025567\n",
      "0  Pclass    -0.996394\n",
      "1     Sex    -2.464776\n"
     ]
    }
   ],
   "source": [
    "#18.Write a Python program to train Logistic Regression and identify important features based on model\n",
    "#coefficients.\n",
    "# Train model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({'Feature': X.columns, 'Coefficient': model.coef_[0]})\n",
    "feature_importance = feature_importance.sort_values(by='Coefficient', ascending=False)\n",
    "\n",
    "print(feature_importance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adbd9b96-d12e-4c06-85b4-dc194c63c7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen‚Äôs Kappa Score: 0.58\n"
     ]
    }
   ],
   "source": [
    "#19.Write a Python program to train Logistic Regression and evaluate its performance using Cohen‚Äôs Kappa\n",
    "#Score.\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "# Train model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Compute Cohen‚Äôs Kappa Score\n",
    "kappa_score = cohen_kappa_score(y_test, y_pred)\n",
    "print(f\"Cohen‚Äôs Kappa Score: {kappa_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4087b9e7-70d4-47bc-aeee-8890b4f282d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#20.Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary\n",
    "#classificatio:\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Generate synthetic dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "202e8717-51b5-4c0c-8eed-db2e0323ac42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solver: liblinear, Accuracy: 0.7821\n",
      "Solver: saga, Accuracy: 0.6983\n",
      "Solver: lbfgs, Accuracy: 0.7989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC DOCTOR\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#21.Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare\n",
    "#their accuracy Write a Python\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "solvers = ['liblinear', 'saga', 'lbfgs']\n",
    "accuracies = {}\n",
    "\n",
    "for solver in solvers:\n",
    "    model = LogisticRegression(solver=solver, max_iter=500, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracies[solver] = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print accuracy comparison\n",
    "for solver, acc in accuracies.items():\n",
    "    print(f\"Solver: {solver}, Accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4fdd5114-d69c-402a-94dc-71d4330ff1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matthews Correlation Coefficient (MCC): 0.6593206923270521\n"
     ]
    }
   ],
   "source": [
    "#22.Write a Python program to train Logistic Regression and evaluate its performance using Matthews\n",
    "#Correlation Coefficient (MCC).\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Generate synthetic dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate Matthews Correlation Coefficient (MCC)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "print(f\"Matthews Correlation Coefficient (MCC): {mcc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c3247d1-ff15-41b9-819c-bad16a9325cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on raw data: 0.7989\n",
      "Accuracy on standardized data: 0.7989\n"
     ]
    }
   ],
   "source": [
    "#23.Write a Python program to train Logistic Regression on both raw and standardized data. Compare their\n",
    "#accuracy to see the impact of feature scaling.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Train on raw data\n",
    "model_raw = LogisticRegression()\n",
    "model_raw.fit(X_train, y_train)\n",
    "acc_raw = accuracy_score(y_test, model_raw.predict(X_test))\n",
    "\n",
    "# Standardize data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train on standardized data\n",
    "model_scaled = LogisticRegression()\n",
    "model_scaled.fit(X_train_scaled, y_train)\n",
    "acc_scaled = accuracy_score(y_test, model_scaled.predict(X_test_scaled))\n",
    "\n",
    "print(f\"Accuracy on raw data: {acc_raw:.4f}\")\n",
    "print(f\"Accuracy on standardized data: {acc_scaled:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea859345-9c63-4eba-81bf-728cb9a00674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C: 0.005994842503189409, Accuracy: 0.8650\n"
     ]
    }
   ],
   "source": [
    "#24.Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using\n",
    "#cross-validation.\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define C values for tuning\n",
    "c_values = np.logspace(-4, 4, 10)\n",
    "\n",
    "# Grid Search for best C\n",
    "param_grid = {'C': c_values}\n",
    "grid_search = GridSearchCV(LogisticRegression(solver='liblinear'), param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Train best model\n",
    "best_C = grid_search.best_params_['C']\n",
    "best_model = LogisticRegression(C=best_C, solver='liblinear')\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Best C: {best_C}, Accuracy: {accuracy:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8c3afad-7099-4647-9064-085e30e39f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Model Accuracy: 0.7989\n"
     ]
    }
   ],
   "source": [
    "#25.Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to\n",
    "#make predictions.\n",
    "import joblib\n",
    "\n",
    "# Train model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(model, 'logistic_regression_model.pkl')\n",
    "\n",
    "# Load the model\n",
    "loaded_model = joblib.load('logistic_regression_model.pkl')\n",
    "\n",
    "# Predict with loaded model\n",
    "y_pred_loaded = loaded_model.predict(X_test)\n",
    "print(f\"Loaded Model Accuracy: {accuracy_score(y_test, y_pred_loaded):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c374255-4dee-4899-9990-b02ebc01ca48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
